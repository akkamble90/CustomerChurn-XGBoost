# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tdwPpgtPulXveEiw7bRwDbLXFINRqgqI
"""

# ===============================================
# üìä XGBoost Mini Project - Customer Churn Prediction (Final Version)
# ===============================================

# Step 1Ô∏è‚É£: Install & Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from google.colab import files
import io

# Step 2Ô∏è‚É£: Upload CSV File
print("üìÇ Please upload your dataset CSV file (e.g., customer_churn.csv)...")
uploaded = files.upload()

# Step 3Ô∏è‚É£: Read the CSV
df = pd.read_csv(io.BytesIO(list(uploaded.values())[0]))
print("\n‚úÖ Dataset Loaded Successfully!")
print("Shape:", df.shape)
print(df.head())

# Step 4Ô∏è‚É£: Drop non-essential ID column (if exists)
if 'CustomerID' in df.columns:
    df = df.drop('CustomerID', axis=1)

# Step 5Ô∏è‚É£: Handle missing values
df = df.replace(" ", np.nan)
df = df.dropna()

# Step 6Ô∏è‚É£: Fix the target column (Churn)
# Convert text labels to numeric if needed
if df['Churn'].dtype == 'object':
    df['Churn'] = df['Churn'].replace({'Yes': 1, 'No': 0, 'Exited': 1, 'Stayed': 0})

# Drop any remaining rows with NaN in Churn
df = df.dropna(subset=['Churn'])

# Convert to int
df['Churn'] = df['Churn'].astype(int)

print("\nUnique values in Churn after cleaning:", df['Churn'].unique())

# Step 7Ô∏è‚É£: Encode Categorical Variables
cat_cols = df.select_dtypes(include=['object']).columns
for col in cat_cols:
    df[col] = df[col].astype('category').cat.codes  # faster encoding

# Step 8Ô∏è‚É£: Split Features & Target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Step 9Ô∏è‚É£: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step üîü: Build XGBoost Model (optimized for speed)
model = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    tree_method='hist'  # change to 'gpu_hist' if Colab GPU enabled
)

# Step 1Ô∏è‚É£1Ô∏è‚É£: Train Model
model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)

# Step 1Ô∏è‚É£2Ô∏è‚É£: Evaluate Model
y_pred = model.predict(X_test)
print("\nüîπ Accuracy:", round(accuracy_score(y_test, y_pred), 4))
print("\nüîπ Classification Report:\n", classification_report(y_test, y_pred))

# Step 1Ô∏è‚É£3Ô∏è‚É£: Confusion Matrix
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Step 1Ô∏è‚É£4Ô∏è‚É£: Feature Importance
plt.figure(figsize=(10,6))
sns.barplot(x=model.feature_importances_, y=X.columns)
plt.title("Feature Importance (XGBoost)")
plt.show()